{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding_size:0\", shape=(10000, 200), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000\n",
    "embedding_size = 200\n",
    "\n",
    "# Q-A 的保存\n",
    "source  = tf.placeholder(dtype=tf.int32,shape=[None,None],name='source_text')\n",
    "target = tf.placeholder(dtype=tf.int32,shape=[None,None],name='target_text')\n",
    "\n",
    "# tf.nn.dynamic_rnn 中有一个参数需要记录出当前batch的每个句子的长度。此处用于存储\n",
    "source_sequence_length = tf.placeholder(dtype=tf.int32,shape=[None,],name='source_seq_length')\n",
    "target_sequence_length = tf.placeholder(dtype=tf.int32,shape=[None,],name='target_seq_length')\n",
    "\n",
    "# 保存于训练的词向量\n",
    "embedding = tf.placeholder(dtype=tf.float32,shape=[vocab_size,embedding_size],name='embedding_size')\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding_lookup:0\", shape=(?, ?, 200), dtype=float32)\n",
      "<tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x000002210EFB8CC0>\n",
      "Tensor(\"rnn/transpose_1:0\", shape=(?, ?, 128), dtype=float32)\n",
      "(LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_3:0' shape=(?, 128) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_4:0' shape=(?, 128) dtype=float32>),)\n"
     ]
    }
   ],
   "source": [
    "# encoder 的建立\n",
    "encoder_embedding  = tf.nn.embedding_lookup(params=embedding,ids=source)\n",
    "\n",
    "lstm_units = 128\n",
    "lstm_layer = 1\n",
    "\n",
    "def get_lstms(nums):\n",
    "    return tf.contrib.rnn.LSTMCell(dtype=tf.float32,num_units=nums,initializer=tf.random_normal_initializer(mean=0.1,stddev=0.1,seed=123))\n",
    "encoder_lstms = tf.contrib.rnn.MultiRNNCell([get_lstms(lstm_units) for _ in range(lstm_layer)])\n",
    "\n",
    "encoder_outputs,encoder_final_state = tf.nn.dynamic_rnn(cell=encoder_lstms,dtype=tf.float32,inputs=encoder_embedding,sequence_length=source_sequence_length)\n",
    "\n",
    "print(encoder_embedding)\n",
    "print(encoder_lstms)\n",
    "print(encoder_outputs)\n",
    "print(encoder_final_state)  #c和h是一样的吗？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "outputs, last_states = tf.nn.dynamic_rnn(  cell=cell,  dtype=tf.float32, sequence_length=x_lengths,  inputs=x)\n",
    "\n",
    "其中cell是RNN节点，比如tf.contrib.rnn.BasicLSTMCel，x是0-padding以后的数据，x_lengths是每个文本的长度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decoder的建立需要注意一下点：\n",
    "1.输入的是encoder的最终状态，\n",
    "2.训练 training 时 decoder的输入时targets； 在infrence时 decoder的输入为上一个lstm结构运算的结构\n",
    "3.training时候使用trainning helper 在做inference时使用greedyEmbeddingHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.contrib.seq2seq.python.ops.helper.TrainingHelper object at 0x000002210F32E588>\n",
      "<tensorflow.python.layers.core.Dense object at 0x000002210F32E4E0>\n",
      "<tensorflow.contrib.seq2seq.python.ops.basic_decoder.BasicDecoder object at 0x000002210F32E5C0>\n",
      "BasicDecoderOutput(rnn_output=<tf.Tensor 'decoder/transpose:0' shape=(?, ?, 1000) dtype=float32>, sample_id=<tf.Tensor 'decoder/transpose_1:0' shape=(?, ?) dtype=int32>)\n",
      "(LSTMStateTuple(c=<tf.Tensor 'decoder/while/Exit_4:0' shape=(?, 128) dtype=float32>, h=<tf.Tensor 'decoder/while/Exit_5:0' shape=(?, 128) dtype=float32>),)\n",
      "Tensor(\"decoder/while/Exit_8:0\", shape=(?,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#decoder的建立\n",
    "decoder_embedding = tf.nn.embedding_lookup(params=embedding,ids=target)\n",
    "\n",
    "# 训练时候的decoder\n",
    "\n",
    "# lstm - cell\n",
    "decoder_lstms = tf.contrib.rnn.MultiRNNCell(cells=[get_lstms(lstm_units) for _ in range(lstm_layer)])\n",
    "# helper\n",
    "train_helper = tf.contrib.seq2seq.TrainingHelper(inputs=decoder_embedding,\n",
    "                                                 sequence_length=target_sequence_length)\n",
    "#output_layer\n",
    "target_vocab_size = 1000\n",
    "output_layer = tf.layers.Dense(units=target_vocab_size)\n",
    "#decoder结构\n",
    "trainning_decoder = tf.contrib.seq2seq.BasicDecoder(cell=decoder_lstms,\n",
    "                                                    helper=train_helper,\n",
    "                                                    initial_state=encoder_final_state,\n",
    "                                                    output_layer=output_layer)\n",
    "max_sequence_length = 25\n",
    "final_outputs,final_state, final_sequence_lengths = tf.contrib.seq2seq.dynamic_decode(decoder=trainning_decoder,\n",
    "                                                                                      impute_finished=True,\n",
    "                                                                                      maximum_iterations=max_sequence_length)\n",
    "\n",
    "print(train_helper)\n",
    "print(output_layer)\n",
    "print(trainning_decoder)\n",
    "print(final_outputs)\n",
    "print(final_state)\n",
    "print(final_sequence_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数说明：\n",
    "train_helper的参数：inputs：embedding向量，lookup之后的矩阵，length：batch中输入的每个sequence的长度矩阵\n",
    "decoder参数 decoder使用初始的BasicDecoder，参数：cell：decoder中的lstms序列，helper 使用的计算方式helper initial_helper是encoder的最终的输出状态，output_layer 是输出层的方式，一般是decens的网络结构，全连接的网络结构。因为输出的时候是一个向量，通过一个向量得到某个值，所以说通过一个全连接层来决定最终的输出是什么。\n",
    "decoder的计算方式这里使用dynamic_decoder的计算方式,最大的迭代次数设置为句子长度的最大值，迭代的可以使用finish向量来进行，返回三个值：final_outputs, final_state, final_sequence_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding_lookup_1:0\", shape=(?, ?, 200), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(decoder_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference时使用的decoder结构，参数可以和trainning时的decoder共享"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以共享的参数有：decoder_embed,decoder_cell,output_layer,需要重新定义decoder_helper,deocer结构以及decoder的计算方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"start_tokens:0\", shape=(50,), dtype=int32)\n",
      "<tensorflow.contrib.seq2seq.python.ops.helper.GreedyEmbeddingHelper object at 0x000002210F32E3C8>\n",
      "<tensorflow.contrib.seq2seq.python.ops.basic_decoder.BasicDecoder object at 0x000002210F32E390>\n",
      "Tensor(\"start_tokens:0\", shape=(50,), dtype=int32)\n",
      "BasicDecoderOutput(rnn_output=<tf.Tensor 'decoder_1/transpose:0' shape=(50, ?, 1000) dtype=float32>, sample_id=<tf.Tensor 'decoder_1/transpose_1:0' shape=(50, ?) dtype=int32>)\n"
     ]
    }
   ],
   "source": [
    "start_id = 0\n",
    "end_id = 3\n",
    "batch_size = 50\n",
    "#start_tokens 是一个向量，end_token是一个标量\n",
    "start_tokens =  tf.tile(tf.constant([start_id], dtype=tf.int32), [batch_size], name=\"start_tokens\")\n",
    "                #tf.tile(tf.constant([start_id], dtype=tf.int32), [batch_size], name=\"start_tokens\")\n",
    "print(start_tokens)\n",
    "inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding=embedding,  # 这里的embedding 不需要做lookup\n",
    "                                                            start_tokens=start_tokens,\n",
    "                                                             end_token=end_id)\n",
    "print(inference_helper)\n",
    "inference_decoder = tf.contrib.seq2seq.BasicDecoder(cell=decoder_lstms,\n",
    "                                                   helper=inference_helper,\n",
    "                                                   initial_state=encoder_final_state,\n",
    "                                                   output_layer=output_layer)\n",
    "\n",
    "print(inference_decoder)\n",
    "final_outputs_infer,_, _ = tf.contrib.seq2seq.dynamic_decode(decoder=inference_decoder,\n",
    "                                                        impute_finished=True,\n",
    "                                                        maximum_iterations=max_sequence_length)\n",
    "\n",
    "print(start_tokens)\n",
    "print(final_outputs_infer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#这里需要注意的是，inferencehelp中的embedding 并不需要做lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainning_outputs = final_outputs\n",
    "# 生成数据进行模型的训练和优化\n",
    "source_length = 788 # source_length = len(source)\n",
    "def get_batches(source,target,batch_size):\n",
    "    for batch_i in range(len(source)):\n",
    "        start_i = batch_i* batch_size\n",
    "\n",
    "        source_batch = source[start_i:start_i+batch_size]\n",
    "        target_batch = target[start_i:start_i+batch_size]\n",
    "\n",
    "        source_sequence_length = []\n",
    "        target_sequence_length = []\n",
    "\n",
    "        for seq in source_batch:\n",
    "            source_sequence_length.append(len(seq))\n",
    "        for seq in rarget_batch:\n",
    "            target_sequence_length.append(len(seq))\n",
    "\n",
    "        yield source_batch,target_batch,source_sequence_length,target_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_outputs =  final_outputs_infer\n",
    "trainin_logits = tf.identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph = tf.Graph()\n",
    "epoch = 10\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch_i in range(epoch):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
